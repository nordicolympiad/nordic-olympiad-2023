\problemname{ChatNOI}
Mary is fascinated by the power of large language models. With all the recent
hype around chat bots and generative AI, she decided to design her own text
generation model called ChatNOI (Chat, but Not Overly Intelligent).

The model is trained on a large document consisting of $n$ words, where it will
learn to recognize patterns in sequences of words. Specifically, for every
distinct sequence of $k$ consecutive words appearing in the document, the model
will keep track of the frequency of words that occur as the next word following
this sequence of $k$ words.

As an example, if the model is trained with the parameter $k = 2$ on the document
\begin{verbatim}
row row to the fishing rocks
out in the ocean they go
a cow is sitting and rowing
and the sun rises
and the sun sets
but the cow and the boat are still there
\end{verbatim}
it will learn that \texttt{row row} is followed once by the word \texttt{to},
\texttt{and the} is followed twice by the word \texttt{sun} and once by the
word \texttt{boat}, \texttt{the sun} is followed once by the word
\texttt{rises} and once by the word \texttt{sets}, and so on. We call the
frequency of a word following a particular sequence of $k$ words the
\textit{likelihood} of that word following that sequence.

Mary has figured out how she can use a trained model to rank the quality of a
given sentence. She looks at every sequence of $k$ consecutive words in the
sentence, and the word that follows that sequence. She then calculates the
likelihood of that word appearing after that sequence, as per the above
definition. The minimum likeliness that she encounters out of all $k$
consecutive words is the quality of that sentence.

Continuing with the above example, the sentence \texttt{cow and the sun rises}
has a quality of $1$, because \texttt{cow and} is followed by \texttt{the} with
a likelihood of $1$, \texttt{and the} is followed by \texttt{sun} with a
likelihood of $2$, and \texttt{the sun} is followed by \texttt{rises} with a
likelihood of $1$, the minimum of which is $1$. Similarly the sentence
\texttt{and the sun} has a quality of $2$ and the sentence \texttt{row to the
boat} has a quality of $0$.

Now that Mary has designed the model and a way to \textit{rank} the quality of
a given sentence, she turns to you for help in using the model to
\textit{generate} sentences. Given an initial part of a sentence and a number
$m$, Mary asks you to finish the last $m$ words of that sentence so that it has
the maximum quality possible according to the trained model. She's pretty
excited so she may even ask you to do this multiple times.

\section*{Input}
The input consists of:
\begin{itemize}
  \item One line with two integers $n$ and $k$ ($1 \leq n \leq 10^5$, $1 \leq k
       \leq 10$, $k < n$), the number of words in the training document and the training
        parameter $k$ as described above.
  \item One line with a sequence of $n$ words $w_1, w_2, \ldots w_n$ ($1 \leq
        \mathrm{length}(w_i) \leq 10$ for each $i$), the training document.
  \item One line with an integer $q$, the number of queries to follow.
  \item $q$ lines, each describing a query consisting of:
    \begin{itemize}
      \item An integer $l$ ($1 \leq l \leq 10^5$), the number of words in the
            initial part of the sentence,
      \item an integer $m$ ($1 \leq m \leq 10^5$), the number of words that
            should be generated to complete the sentence, and
      \item a sequence of $l$ words $u_1, u_2, \ldots, u_l$ ($1 \leq
            \mathrm{length}(u_i) \leq 10$ for each $i$), the initial part of the sentence.
    \end{itemize}
\end{itemize}

\section*{Output}

\section*{Scoring}
\begin{tabular}{|l|l|l|}
    \hline
    Group & Points & Constraints \\ \hline
    %1     &  10  & $N \leq 10^6$, $K = 1$, $M \leq 10^6$ \\ \hline
\end{tabular}

